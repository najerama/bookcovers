{"cells": [{"cell_type": "code", "execution_count": 47, "metadata": {}, "outputs": [], "source": "import operator\nimport sys\nimport pyspark\nimport numpy as np\nfrom pyspark.sql import SparkSession\nfrom collections import defaultdict"}, {"cell_type": "code", "execution_count": 48, "metadata": {}, "outputs": [], "source": "PREFIX = \"gs://book-covers-e6893/\"\nreviews_file = \"Books_5.json\"\nmeta_file = \"meta_Books.json\""}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": "spark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"covers\") \\\n    .getOrCreate();\nsc = spark.sparkContext"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": "reviews = spark.read.json(PREFIX+reviews_file)"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": "metadata = spark.read.json(PREFIX+meta_file)"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- asin: string (nullable = true)\n |-- image: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- overall: double (nullable = true)\n |-- reviewText: string (nullable = true)\n |-- reviewTime: string (nullable = true)\n |-- reviewerID: string (nullable = true)\n |-- reviewerName: string (nullable = true)\n |-- style: struct (nullable = true)\n |    |-- Color:: string (nullable = true)\n |    |-- Format:: string (nullable = true)\n |    |-- Package Quantity:: string (nullable = true)\n |    |-- Size:: string (nullable = true)\n |    |-- Style Name:: string (nullable = true)\n |    |-- Style:: string (nullable = true)\n |-- summary: string (nullable = true)\n |-- unixReviewTime: long (nullable = true)\n |-- verified: boolean (nullable = true)\n |-- vote: string (nullable = true)\n\n"}], "source": "reviews.printSchema()"}, {"cell_type": "code", "execution_count": 54, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- also_buy: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- also_view: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- asin: string (nullable = true)\n |-- brand: string (nullable = true)\n |-- category: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- date: string (nullable = true)\n |-- description: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- details: string (nullable = true)\n |-- feature: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- fit: string (nullable = true)\n |-- image: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- main_cat: string (nullable = true)\n |-- price: string (nullable = true)\n |-- rank: string (nullable = true)\n |-- similar_item: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- asin: string (nullable = true)\n |    |    |-- features: struct (nullable = true)\n |    |    |    |-- Are Batteries Required: string (nullable = true)\n |    |    |    |-- Batteries are Included: string (nullable = true)\n |    |    |    |-- Bible Type: string (nullable = true)\n |    |    |    |-- Color: string (nullable = true)\n |    |    |    |-- Compatible Phone Models: string (nullable = true)\n |    |    |    |-- Cover Material: string (nullable = true)\n |    |    |    |-- Customer Rating: string (nullable = true)\n |    |    |    |-- Edition Number: string (nullable = true)\n |    |    |    |-- Item Dimensions: string (nullable = true)\n |    |    |    |-- Item Weight: string (nullable = true)\n |    |    |    |-- Material Type: string (nullable = true)\n |    |    |    |-- Number of Items: string (nullable = true)\n |    |    |    |-- Price: string (nullable = true)\n |    |    |    |-- Series Number: string (nullable = true)\n |    |    |    |-- Shipping: string (nullable = true)\n |    |    |    |-- Size: string (nullable = true)\n |    |    |    |-- Sold By: string (nullable = true)\n |    |    |    |-- Source Country Code: string (nullable = true)\n |    |    |-- href: string (nullable = true)\n |    |    |-- img: string (nullable = true)\n |    |    |-- name: string (nullable = true)\n |-- tech1: struct (nullable = true)\n |    |-- ABPA Partslink Number: string (nullable = true)\n |    |-- Assembled Height: string (nullable = true)\n |    |-- Assembled Length: string (nullable = true)\n |    |-- Assembled Width: string (nullable = true)\n |    |-- Backing: string (nullable = true)\n |    |-- Batteries Included?: string (nullable = true)\n |    |-- Batteries Required?: string (nullable = true)\n |    |-- Binding Edge: string (nullable = true)\n |    |-- Brand: string (nullable = true)\n |    |-- Brand Name: string (nullable = true)\n |    |-- Brightness Rating: string (nullable = true)\n |    |-- CAPA Certified: string (nullable = true)\n |    |-- California residents: string (nullable = true)\n |    |-- Color: string (nullable = true)\n |    |-- Cover Included: string (nullable = true)\n |    |-- Cover Material: string (nullable = true)\n |    |-- Discontinued by manufacturer: string (nullable = true)\n |    |-- Exterior: string (nullable = true)\n |    |-- Folding: string (nullable = true)\n |    |-- Item Package Quantity: string (nullable = true)\n |    |-- Item Weight: string (nullable = true)\n |    |-- Item model number: string (nullable = true)\n |    |-- Manufacturer Part Number: string (nullable = true)\n |    |-- Material: string (nullable = true)\n |    |-- Material Type: string (nullable = true)\n |    |-- Model: string (nullable = true)\n |    |-- Number of Items: string (nullable = true)\n |    |-- Package Dimensions: string (nullable = true)\n |    |-- Paper Finish: string (nullable = true)\n |    |-- Part Number: string (nullable = true)\n |    |-- Product Dimensions: string (nullable = true)\n |    |-- Safety Rating: string (nullable = true)\n |    |-- Sheet Size: string (nullable = true)\n |    |-- Size: string (nullable = true)\n |    |-- Vehicle Service Type: string (nullable = true)\n |    |-- Weight: string (nullable = true)\n |    |-- Whiteness: string (nullable = true)\n |-- title: string (nullable = true)\n\n"}], "source": "metadata.printSchema()"}, {"cell_type": "code", "execution_count": 101, "metadata": {}, "outputs": [], "source": "metadata_small = metadata.select(\"asin\",\"rank\",\"also_buy\")"}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [], "source": "r = reviews.where(reviews.asin == '0001713353').rdd.map(lambda entry : (entry['asin'], (entry['overall'], 1)))"}, {"cell_type": "code", "execution_count": 26, "metadata": {}, "outputs": [{"data": {"text/plain": "[('0001713353', (5.0, 1)),\n ('0001713353', (5.0, 1)),\n ('0001713353', (5.0, 1)),\n ('0001713353', (5.0, 1)),\n ('0001713353', (5.0, 1)),\n ('0001713353', (5.0, 1)),\n ('0001713353', (5.0, 1)),\n ('0001713353', (5.0, 1)),\n ('0001713353', (5.0, 1)),\n ('0001713353', (5.0, 1)),\n ('0001713353', (5.0, 1)),\n ('0001713353', (5.0, 1)),\n ('0001713353', (3.0, 1)),\n ('0001713353', (5.0, 1)),\n ('0001713353', (5.0, 1)),\n ('0001713353', (5.0, 1)),\n ('0001713353', (5.0, 1)),\n ('0001713353', (5.0, 1)),\n ('0001713353', (5.0, 1)),\n ('0001713353', (5.0, 1))]"}, "execution_count": 26, "metadata": {}, "output_type": "execute_result"}], "source": "r.take(100)"}, {"cell_type": "code", "execution_count": 28, "metadata": {}, "outputs": [], "source": "consolidated = reviews.rdd.map(lambda entry : (entry['asin'], (entry['overall'], 1))).reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1]))"}, {"cell_type": "code", "execution_count": 31, "metadata": {}, "outputs": [{"data": {"text/plain": "[('0005377188', 3.764705882352941, 64.0),\n ('0006490018', 4.0754716981132075, 216.0)]"}, "execution_count": 31, "metadata": {}, "output_type": "execute_result"}], "source": "consolidated.map(lambda x: (x[0], x[1][0]/x[1][1], x[1][0])).take(2)"}, {"cell_type": "code", "execution_count": 69, "metadata": {}, "outputs": [], "source": "bucket = \"book-covers-e6893\"    # TODO : replace with your own bucket name\noutput_directory = 'gs://{}/hadoop/tmp/bigquery/pyspark_output/book_metadata'.format(bucket)\n# consolidated.map(lambda x: (x[0], x[1][0]/x[1][1], x[1][0])).toDF().write.save(output_directory, format=\"json\")"}, {"cell_type": "code", "execution_count": 50, "metadata": {}, "outputs": [{"data": {"text/plain": "True"}, "execution_count": 50, "metadata": {}, "output_type": "execute_result"}], "source": "from google.cloud import bigquery\nimport subprocess\n\noutput_dataset = 'book_metadata' \noutput_table = 'book_ratings'\nfiles = output_directory + '/part-*'\nsubprocess.check_call(\n    'bq load --source_format NEWLINE_DELIMITED_JSON '\n    '--replace '\n    '--autodetect '\n    '{dataset}.{table} {files}'.format(\n        dataset=output_dataset, table=output_table, files=files\n    ).split())\noutput_path = sc._jvm.org.apache.hadoop.fs.Path(output_directory)\noutput_path.getFileSystem(sc._jsc.hadoopConfiguration()).delete(output_path, True)"}, {"cell_type": "code", "execution_count": 42, "metadata": {}, "outputs": [], "source": "df = consolidated.map(lambda x: (x[0], x[1][0]/x[1][1], x[1][0])).toDF()"}, {"cell_type": "code", "execution_count": 46, "metadata": {}, "outputs": [], "source": "dff = df.withColumnRenamed(\"_1\", \"ASIN\").withColumnRenamed(\"_2\", \"rating\").withColumnRenamed(\"_3\", \"total_reviews\")"}, {"cell_type": "code", "execution_count": 49, "metadata": {}, "outputs": [], "source": "dff.write.save(output_directory, format=\"json\", mode=\"overwrite\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 53, "metadata": {}, "outputs": [], "source": "import numpy as np\nfrom sklearn.neighbors import NearestNeighbors"}, {"cell_type": "code", "execution_count": 102, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- asin: string (nullable = true)\n |-- rank: string (nullable = true)\n |-- also_buy: array (nullable = true)\n |    |-- element: string (containsNull = true)\n\n"}], "source": "metadata_small.printSchema()"}, {"cell_type": "code", "execution_count": 128, "metadata": {}, "outputs": [], "source": "metadata_small = metadata.select(\"asin\",\"rank\",\"also_buy\")"}, {"cell_type": "code", "execution_count": 137, "metadata": {}, "outputs": [], "source": "import re\nmetadata_small0 = metadata_small.rdd.map(lambda x: (x[0], \"0\", x[2]) if not x[1] else (x[0], x[1], x[2]))\nmetadata_small2 = metadata_small0.map(lambda x: (x[0], re.sub(\"[^0-9]\", \"\", x[1]).replace(\",\", ''), x[2]))"}, {"cell_type": "code", "execution_count": 138, "metadata": {}, "outputs": [], "source": "metadata_small3 = metadata_small2.map(lambda x: (x[0], x[1], \"\") if x[2] == None else (x[0], x[1], \",\".join(x[2])))"}, {"cell_type": "code", "execution_count": 144, "metadata": {}, "outputs": [], "source": "metadata_small4 = metadata_small3.map(lambda x: (x[0], 0, x[2]) if not x[1] else (x[0], int(x[1]), x[2]))"}, {"cell_type": "code", "execution_count": 145, "metadata": {}, "outputs": [], "source": "metadata_df = metadata_small4.toDF()"}, {"cell_type": "code", "execution_count": 146, "metadata": {}, "outputs": [], "source": "metadata_df = metadata_df.withColumnRenamed(\"_1\", \"ASIN\").withColumnRenamed(\"_2\", \"ranking\").withColumnRenamed(\"_3\", \"also_bought\")"}, {"cell_type": "code", "execution_count": 147, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------+--------+--------------------+\n|      ASIN| ranking|         also_bought|\n+----------+--------+--------------------+\n|0000092878| 1349781|0669009075,B000K2...|\n|000047715X| 1702625|                    |\n|0000004545| 6291012|                    |\n|0000013765| 2384057|                    |\n|0000000116|11735726|                    |\n|0000555010| 2906939|0323056962,012397...|\n|0000477141| 2236549|                    |\n|0000230022| 2566783|1492630519,007181...|\n|0000038504| 2505873|                    |\n|0000001589| 4368310|                    |\n+----------+--------+--------------------+\nonly showing top 10 rows\n\n"}], "source": "metadata_df.show(10)"}, {"cell_type": "code", "execution_count": 148, "metadata": {}, "outputs": [], "source": "metadata_df.write.save(output_directory, format=\"json\", mode=\"overwrite\")"}, {"cell_type": "code", "execution_count": 149, "metadata": {}, "outputs": [{"data": {"text/plain": "True"}, "execution_count": 149, "metadata": {}, "output_type": "execute_result"}], "source": "from google.cloud import bigquery\nimport subprocess\n\noutput_dataset = 'book_metadata' \noutput_table = 'book_rankings'\nfiles = output_directory + '/part-*'\nsubprocess.check_call(\n    'bq load --source_format NEWLINE_DELIMITED_JSON '\n    '--replace '\n    '--autodetect '\n    '{dataset}.{table} {files}'.format(\n        dataset=output_dataset, table=output_table, files=files\n    ).split())\noutput_path = sc._jvm.org.apache.hadoop.fs.Path(output_directory)\noutput_path.getFileSystem(sc._jsc.hadoopConfiguration()).delete(output_path, True)"}, {"cell_type": "code", "execution_count": 49, "metadata": {}, "outputs": [], "source": "from google.cloud import bigquery\nimport subprocess\nclient = bigquery.Client()"}, {"cell_type": "code", "execution_count": 50, "metadata": {}, "outputs": [], "source": "sql = \"\"\"\nSELECT *\nFROM\n    `eecs-e6893-book-cover.book_cover_data.image_data`\n\"\"\"\ndf = client.query(sql).to_dataframe()"}, {"cell_type": "code", "execution_count": 51, "metadata": {}, "outputs": [], "source": "sdf = spark.createDataFrame(df)"}, {"cell_type": "code", "execution_count": 52, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- file_number: string (nullable = true)\n |-- top_color_R: double (nullable = true)\n |-- top_color_G: double (nullable = true)\n |-- top_color_B: double (nullable = true)\n |-- brightness: double (nullable = true)\n |-- colorfullness: double (nullable = true)\n\n"}], "source": "sdf.printSchema()"}, {"cell_type": "code", "execution_count": 53, "metadata": {}, "outputs": [], "source": "data = sdf.rdd.map(lambda x : (x[0], (x[1], x[2], x[3], x[4], x[5]))).collect()"}, {"cell_type": "code", "execution_count": 54, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "57000\n"}, {"data": {"text/plain": "('0557080398', (0.56, 0.56, 0.56, 8.35, 0.0))"}, "execution_count": 54, "metadata": {}, "output_type": "execute_result"}], "source": "print(len(data))\ndata[0]"}, {"cell_type": "code", "execution_count": 55, "metadata": {}, "outputs": [], "source": "test = sdf.rdd.map(lambda x : (x[0], (x[1], x[2], x[3], x[4], x[5]))).toDF()"}, {"cell_type": "code", "execution_count": 56, "metadata": {}, "outputs": [], "source": "from pyspark.sql import Window\nfrom pyspark.sql import functions as F\nwindow = Window.orderBy(F.col('_1'))\ntmp = test.withColumn('index', F.row_number().over(window)).withColumnRenamed(\"_1\", \"ASIN\").withColumnRenamed(\"_2\", \"features\").select(\"index\", \"ASIN\", \"features\").rdd.map(lambda x : (x[0], x[1], (x[2][0], x[2][1], x[2][2], x[2][3], x[2][4]))).collect()"}, {"cell_type": "code", "execution_count": 57, "metadata": {}, "outputs": [{"data": {"text/plain": "(251.97, 251.97, 251.97, 197.87, 0.0)"}, "execution_count": 57, "metadata": {}, "output_type": "execute_result"}], "source": "test2 = sdf.rdd.map(lambda x : (x[1], x[2], x[3], x[4], x[5])).collect()\ntest2[3]"}, {"cell_type": "code", "execution_count": 58, "metadata": {}, "outputs": [{"data": {"text/plain": "(1.25, 1.25, 1.25, 7.27, 0.0)"}, "execution_count": 58, "metadata": {}, "output_type": "execute_result"}], "source": "test2[103]"}, {"cell_type": "code", "execution_count": 59, "metadata": {}, "outputs": [], "source": "from sklearn.neighbors import NearestNeighbors"}, {"cell_type": "code", "execution_count": 60, "metadata": {}, "outputs": [{"data": {"text/plain": "NearestNeighbors(algorithm='auto', leaf_size=30, metric='minkowski',\n         metric_params=None, n_jobs=1, n_neighbors=5, p=2, radius=1.0)"}, "execution_count": 60, "metadata": {}, "output_type": "execute_result"}], "source": "knn = NearestNeighbors()\nknn.fit(test2)"}, {"cell_type": "code", "execution_count": 61, "metadata": {}, "outputs": [{"data": {"text/plain": "array([[ 0.        ,  7.70909203,  9.21531877, 10.16878557, 10.47136094]])"}, "execution_count": 61, "metadata": {}, "output_type": "execute_result"}], "source": "d,i = knn.kneighbors([[201.26, 135.58, 135.58, 132.64, 40.42]])\nd"}, {"cell_type": "code", "execution_count": 62, "metadata": {}, "outputs": [{"data": {"text/plain": "array([['0670885517', (201.26, 135.58, 135.58, 132.64, 40.42)],\n       ['1570035121', (201.61, 132.15, 132.15, 132.27, 34.45)],\n       ['157344331X', (199.21, 138.32, 138.32, 139.02, 35.42)],\n       ['1449466338', (197.91, 142.18, 142.18, 130.4, 40.21)],\n       ['1565232577', (194.35, 140.26, 140.26, 129.5, 37.55)]],\n      dtype=object)"}, "execution_count": 62, "metadata": {}, "output_type": "execute_result"}], "source": "np.array(data)[i[0]]"}, {"cell_type": "code", "execution_count": 63, "metadata": {}, "outputs": [{"data": {"text/plain": "array([[55159, 34556, 56260, 27511, 24685]])"}, "execution_count": 63, "metadata": {}, "output_type": "execute_result"}], "source": "i"}, {"cell_type": "code", "execution_count": 64, "metadata": {}, "outputs": [{"data": {"text/plain": "(57000, 2)"}, "execution_count": 64, "metadata": {}, "output_type": "execute_result"}], "source": "np.shape(np.array(data))"}, {"cell_type": "code", "execution_count": 65, "metadata": {}, "outputs": [], "source": "import pickle \nknnPickle = open('knn-model', 'wb') \npickle.dump(knn, knnPickle)"}, {"cell_type": "code", "execution_count": 66, "metadata": {}, "outputs": [], "source": "import cloudstorage as gcs"}, {"cell_type": "code", "execution_count": 73, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "auth.json  dev\t   initrd.img\t   lib64       opt   sbin  usr\nbin\t   etc\t   initrd.img.old  lost+found  proc  srv   var\nboot\t   hadoop  knn-model.jlib  media       root  sys   vmlinuz\ncopyright  home    lib\t\t   mnt\t       run   tmp   vmlinuz.old\n"}], "source": "from sklearn.externals import joblib \nknnPickle = open('knn-model.jlib', 'wb') \njoblib.dump(knn, knnPickle)\nknnPickle.close()\n!ls"}, {"cell_type": "code", "execution_count": 71, "metadata": {}, "outputs": [], "source": "output_path = sc._jvm.org.apache.hadoop.fs.Path(\"gs://book-covers-e6893/\")\nfs = output_path.getFileSystem(sc._jsc.hadoopConfiguration())\nfs.moveFromLocalFile(sc._jvm.org.apache.hadoop.fs.Path(\"knn-model.jlib\"), output_path)"}, {"cell_type": "code", "execution_count": 74, "metadata": {}, "outputs": [], "source": "import pandas as pd \npd.DataFrame(data).to_csv(\"indexed_books.csv\")"}, {"cell_type": "code", "execution_count": 70, "metadata": {}, "outputs": [{"data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0557080398</td>\n      <td>(0.56, 0.56, 0.56, 8.35, 0.0)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2951246005</td>\n      <td>(254.82, 254.82, 254.82, 254.3, 0.0)</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>094020813X</td>\n      <td>(248.25, 248.25, 248.25, 176.2, 0.0)</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1495972666</td>\n      <td>(251.97, 251.97, 251.97, 197.87, 0.0)</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1570272670</td>\n      <td>(12.7, 12.7, 12.7, 58.66, 0.0)</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>56995</th>\n      <td>3037781831</td>\n      <td>(246.34, 245.51, 245.51, 237.41, 1.56)</td>\n    </tr>\n    <tr>\n      <th>56996</th>\n      <td>0692489428</td>\n      <td>(250.16, 250.59, 250.59, 209.13, 1.56)</td>\n    </tr>\n    <tr>\n      <th>56997</th>\n      <td>0300213964</td>\n      <td>(132.93, 132.62, 132.62, 108.1, 1.81)</td>\n    </tr>\n    <tr>\n      <th>56998</th>\n      <td>0002182718</td>\n      <td>(105.76, 53.52, 53.52, 150.01412667233552, 43....</td>\n    </tr>\n    <tr>\n      <th>56999</th>\n      <td>0001484524</td>\n      <td>(249.3, 245.0, 245.0, 188.15822335090087, 68.3...</td>\n    </tr>\n  </tbody>\n</table>\n<p>57000 rows \u00d7 2 columns</p>\n</div>", "text/plain": "                0                                                  1\n0      0557080398                      (0.56, 0.56, 0.56, 8.35, 0.0)\n1      2951246005               (254.82, 254.82, 254.82, 254.3, 0.0)\n2      094020813X               (248.25, 248.25, 248.25, 176.2, 0.0)\n3      1495972666              (251.97, 251.97, 251.97, 197.87, 0.0)\n4      1570272670                     (12.7, 12.7, 12.7, 58.66, 0.0)\n...           ...                                                ...\n56995  3037781831             (246.34, 245.51, 245.51, 237.41, 1.56)\n56996  0692489428             (250.16, 250.59, 250.59, 209.13, 1.56)\n56997  0300213964              (132.93, 132.62, 132.62, 108.1, 1.81)\n56998  0002182718  (105.76, 53.52, 53.52, 150.01412667233552, 43....\n56999  0001484524  (249.3, 245.0, 245.0, 188.15822335090087, 68.3...\n\n[57000 rows x 2 columns]"}, "execution_count": 70, "metadata": {}, "output_type": "execute_result"}], "source": "pd.DataFrame(data)"}, {"cell_type": "code", "execution_count": 72, "metadata": {}, "outputs": [], "source": "fs.moveFromLocalFile(sc._jvm.org.apache.hadoop.fs.Path(\"indexed_books.csv\"), output_path)"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.5"}}, "nbformat": 4, "nbformat_minor": 2}